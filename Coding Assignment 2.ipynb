{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5f48c1",
   "metadata": {},
   "source": [
    "<h1> Task 1:  CIFAR-10 </h1>\n",
    "\n",
    "In this assignment, you will be building your own architecture for a (convolutional) neural network, and you will try to get a reasonable model.  You may use the code below, or you may build your own architecture.  Create an architecture that receives over 70% on accuracy on the testing set for CIFAR-10.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1bc3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#you may need to pip install the appropriate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef1b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##preprocessing, it is easy to get the data via torchvision.\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34263868",
   "metadata": {},
   "source": [
    "Let's show some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37232432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26ae04-b767-4c4d-86d6-98aec24ab798",
   "metadata": {},
   "source": [
    "The below code is the one we used in class, which yielded an accuracy of around 56% on the testing set.  You will be creating your own CNN to get better accuracy.  You may either start from scratch of use the CNN below as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd28c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net_Improved(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1st Convolutional Block: 3 -> 64 filters\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 2nd Convolutional Block: 64 -> 128 filters\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Output size after two 2x2 max-pools (32x32 -> 16x16 -> 8x8) \n",
    "        # is 128 channels * 8 * 8.\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st Block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x)))) # 64 channels, 16x16\n",
    "        \n",
    "        # 2nd Block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x)))) # 128 channels, 8x8\n",
    "        \n",
    "        # Flatten and Fully Connected Layers\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net_Improved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c97b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb83a21",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305524da-b00c-47bf-a3de-c545c828c56a",
   "metadata": {},
   "source": [
    "The code below trains the network, but you may of course use your own code/your own version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f602454-0d70-4e34-b6d8-4e39e9407e72",
   "metadata": {},
   "source": [
    "<h1>Include a chart of the training loss as a function of the training.  Your chart should also include the accuracy.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ace16-d7c4-451e-ad05-7e6aa8b5ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "     running_loss = 0.0\n",
    "     for i, data in enumerate(trainloader, 0): \n",
    "         running_loss += loss.item()\n",
    "         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "             running_loss = 0.0\n",
    " \n",
    " print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bad17b-d821-4841-8d22-09a929edc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set (images)\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4db6c-f649-4611-bcf8-e4a546cd35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c9f56-f306-42ec-9d0f-f2ecacf1e232",
   "metadata": {},
   "source": [
    "<h1> Task 2:  Language Model </h1>\n",
    "\n",
    "Consider the language model from class.  This was the one on Friday, April 4th.  Modify the \"intents.json\" and train a chatbot that can respond to more queries.  You may use the architecture in class, or implement the transformer architecture (optional).  Please include an output of your conversation with the bot with the following inputs:\n",
    "\n",
    "1.  \"Hi.\"\n",
    "2.  \"How are you doing?\"\n",
    "3.  \"What is the weather like today.\"\n",
    "4.  \"What are your plans for the weekend?\"\n",
    "5.  \"What are your plans for the summer?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c86d5d-4f65-4655-8fe8-11dd75aca92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Helper function for tokenization\n",
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "# Helper function for stemming\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "# Helper function to create Bag-of-Words\n",
    "def bag_of_words(tokenized_sentence, all_words):\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
    "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bag[idx] = 1.0\n",
    "    return bag\n",
    "\n",
    "# Load intents.json\n",
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = [] # list of (pattern, tag)\n",
    "\n",
    "# Loop through each intent and pattern\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern)\n",
    "        all_words.extend(w)\n",
    "        xy.append((w, tag))\n",
    "\n",
    "# Filter and stem all words\n",
    "ignore_words = ['?', '!', '.', ',']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "# Sort and remove duplicates\n",
    "all_words = sorted(list(set(all_words)))\n",
    "tags = sorted(list(set(tags)))\n",
    "\n",
    "# Create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = torch.from_numpy(X_train)\n",
    "        self.y_data = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# Hyperparameters and DataLoader\n",
    "batch_size = 8\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "input_size = len(X_train[0]) \n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000 # Increased for better performance\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a59cf7-cef4-4243-b4fa-758dd3d86fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # 3 fully connected layers\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU() # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax at the end (CrossEntropyLoss handles this)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6752b-75d4-4e81-9467-a2316a9b0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model, loss, and optimizer\n",
    "model = NeuralNet(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f'Final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c4b48-c8e8-48b0-a9db-90c407566b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Chat loop\n",
    "print(\"Let's chat! Type 'quit' to exit.\")\n",
    "while True:\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "\n",
    "    # 1. Preprocess input\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X)\n",
    "\n",
    "    # 2. Get model prediction\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "    \n",
    "    # 3. Get predicted tag\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    # 4. Check confidence (optional, but good practice)\n",
    "    # The output logit needs softmax to become probability\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "\n",
    "    if prob.item() > 0.75: # Confidence threshold\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                # 5. Output response\n",
    "                response = random.choice(intent['responses'])\n",
    "                print(f\"Bot: {response}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Bot: I do not understand...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9b379-11a4-4b4d-870b-dbc68ebf5de2",
   "metadata": {},
   "source": [
    "<h1> Task 3: Gradient boosting </h1>\n",
    "\n",
    "In this task, you will be using a gradient boosting method to predict flight departure delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe6cfe-30d8-4163-ba95-75db629c168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier #you may need to pip install this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161feeb-5998-4a8f-b54d-a4498f9865bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"flight_delays_train.csv\")\n",
    "test = pd.read_csv(\"flight_delays_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceece12-335f-44a6-8461-a8404aaee725",
   "metadata": {},
   "source": [
    "<h2> Given flight departure time, carrierâ€™s code, departure airport, destination location, and flight distance, you have to predict departure delay for more than 15 minutes. </h2>\n",
    "The code below uses only two features (Distance and departure time).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4e19d-a8f1-45b0-b77d-2c1187141508",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"Distance\", \"DepTime\"]].values\n",
    "y_train = train[\"dep_delayed_15min\"].map({\"Y\": 1, \"N\": 0}).values\n",
    "X_test = test[[\"Distance\", \"DepTime\"]].values\n",
    "\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a125a-9d05-47ab-a4a7-4d0953921563",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(seed=17)\n",
    "\n",
    "xgb_model.fit(X_train_part, y_train_part)\n",
    "xgb_valid_pred = xgb_model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995586a-0109-4471-8f55-a2bf02b7373d",
   "metadata": {},
   "source": [
    "<h2> Your task!</h2>\n",
    "Try to create a better model!  The threshold for what a 'better' model is is up to you - you could spend a lot of time making a good model. The evaluation metric is the ROC AUC (you can read more about it via a quick Google search).  Here are some starting points: try to modify which features to use, and also you may attempt to create new features.  Evaluate your model on the testing set (see the above \"flight_delays_test.csv\").  This (was) actually a Kaggle competition: https://www.kaggle.com/c/flight-delays-spring-2018.  If you want, you can submit your results there to see how well you did (notably, the testing labels are on purpose not part of this data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836eda2e-03a2-4aef-88b5-173354372a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable from 'Y'/'N' to 1/0\n",
    "train[\"dep_delayed_15min\"] = train[\"dep_delayed_15min\"].map({\"Y\": 1, \"N\": 0})\n",
    "y_train = train[\"dep_delayed_15min\"].values\n",
    "\n",
    "# Combine train and test for consistent feature engineering (but keep labels separate!)\n",
    "full_data = pd.concat([train.drop(\"dep_delayed_15min\", axis=1), test], ignore_index=True)\n",
    "\n",
    "# 1. Feature Engineering: Departure Hour\n",
    "# DepTime is in HHMM format (e.g., 1934 for 7:34 PM). Extract the hour.\n",
    "full_data['DepHour'] = full_data['DepTime'] // 100\n",
    "# Handle case where DepTime is 2400 (midnight) or near (e.g., 1, 100, 2400) - set them to 0 or 23\n",
    "full_data.loc[full_data['DepHour'] == 24, 'DepHour'] = 0\n",
    "\n",
    "# 2. Categorical Features for Target Encoding\n",
    "cat_features = ['UniqueCarrier', 'Origin', 'Dest']\n",
    "\n",
    "# Target Encoding (Mean of the target for each category)\n",
    "# We calculate the mean for the training data only to avoid data leakage.\n",
    "for feature in cat_features:\n",
    "    # Calculate the mean target for each category in the training set\n",
    "    target_mean = train.groupby(feature)['dep_delayed_15min'].mean()\n",
    "    \n",
    "    # Create the new encoded feature name\n",
    "    encoded_feature_name = f'{feature}_Target_Encoded'\n",
    "    \n",
    "    # Map the mean values to the full dataset\n",
    "    full_data[encoded_feature_name] = full_data[feature].map(target_mean)\n",
    "\n",
    "    # Fill NaNs in test data (categories not seen in train) with the global mean\n",
    "    global_mean = train['dep_delayed_15min'].mean()\n",
    "    full_data[encoded_feature_name].fillna(global_mean, inplace=True)\n",
    "    \n",
    "# 3. Final Feature Set\n",
    "# Note: XGBoost can handle the numerical representation of Month, DayofMonth, etc.\n",
    "# But for better performance, they should be treated as categorical or binned.\n",
    "# We'll use the simplest effective set for improvement:\n",
    "features = ['Distance', 'DepTime', 'DepHour', \n",
    "            'UniqueCarrier_Target_Encoded', 'Origin_Target_Encoded', 'Dest_Target_Encoded',\n",
    "            'Month', 'DayOfWeek']\n",
    "\n",
    "# Split back into train and test\n",
    "X_train = full_data.iloc[:len(train)][features].values\n",
    "X_test = full_data.iloc[len(train):][features].values\n",
    "\n",
    "# Re-define train/validation split\n",
    "X_train_part, X_valid, y_train_part, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788fc86-66ba-4089-b357-5a8507518262",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(seed=17)\n",
    "\n",
    "xgb_model.fit(X_train_part, y_train_part)\n",
    "xgb_valid_pred = xgb_model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31a025-7138-4f4d-b62a-9a5650fa5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_valid, xgb_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c519235-5498-4677-b0a9-4eda2b87eb5f",
   "metadata": {},
   "source": [
    "<h2> Task 4: Some questions (no code) </h2>\n",
    " \n",
    "1. What was the architecture you used in the CNN for CIFAR-10?  How many features did you have?\n",
    "\n",
    "2. How would you attempt to improve your model in (1)?\n",
    "\n",
    "3. When do you think gradient-boosting methods would perform well, and when would they perform poorly?\n",
    "\n",
    "4. (Optional) Compare XGBoost to logistic regression in (3), and implement transformers in (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203fee97-e038-45d5-bdc2-0b0d86319563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     1. Convolutions --> Block Normalization -> ReLU --> Convolutions --> Batch Normalization \n",
    "#     --> ReLU --> max the pool for two blocks (3-->64 and 64-->128). There were 8192 features.\n",
    "#     2. May want to use Deep Residual networks\n",
    "#     3. GB models perform well in structured data, data with mixed features, and alarge data set.\n",
    "#     GB models do not perform well under unstructured data, data involving real time prediction, \n",
    "#     and extremely large data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37710a-54eb-4a12-859e-ca1579bfe5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
